---
title: "Evaluating the Benefits of Sample Splitting for Double Machine Learning"
author: "Michael Sarkis, Jack McCarthy"
date: "4/24/2022"
output: pdf_document
---

```{r setup, include=F}
knitr::opts_chunk$set(echo=T)
```

```{r, include=F}
library(tidyverse)
library(MASS)
library(randomForest)
library(latex2exp)
library(xgboost)
library(ggpubr)
```

# Double Machine Learning

In this section we will replicate the methods put forth in the paper by 
Chernozhukov et al., dubbed "CCDDHNR", which establishes an unbiased Bayesian 
machine learning framework for treatment effect estimation. This work deals 
with treatment effect estimation for complex data which has a high number of 
confounding parameters relative to a low dimensional treatment effect.

### Simulated Dataset

First we will generate a toy dataset on which we will test each method. We will 
generate data according to the following partial-linear model, which relates 
the treatment linearly to the response and the covariates non-linearly to both 
the response and the treatment:

$$
\begin{aligned}
Y_i = D_i\theta + g_0(X_i) + \epsilon_i, \quad &\epsilon_i \sim N(0, 1), \\
D_i = m_0(X_i) + \tau_i, \quad \quad &\tau_i \sim N(0, 1),
\end{aligned}
$$

where $Y$ is the outcome, $D$ is the treatment, and $X$ is the vector of 
covariates. $g_0(X)$ and $m_0(X)$ relate the covariates to the value of the 
response and the treatment respectively. We define these "nuisance" functions 
to be the following non-linear form:

$$
\begin{aligned}
g_0(x) = \sin(x_1) + \cos(x_2) + \sigma(x_3), \\
m_0(x) = \sin(x_4) + \cos(x_5) + \sigma(x_6),
\end{aligned}
$$

where $\sigma(x)$ is the sigmoid function: 

$$\sigma(x) = \frac{1}{1 + e^{-x}}.$$

We also define $X_i \sim N(0, \Sigma)$ where $\Sigma_{jk} = 0.5^{|j-k|}.$

We will generate a toy data set assuming a continuous response and treatment, 250 total observations, 
a 100-dimensional covariate vector, and a true value of $\theta = 0.5$.

```{r, include=F}
sigmoid <- function(x) {
  1 / (1 + exp(-x))
}

g_0 <- function(X) {
  sin(X[,1]) + cos(X[,2]) + sigmoid(X[,3])
}

m_0 <- function(X) {
  sin(X[,4]) + cos(X[,5]) + sigmoid(X[,6])
}
```

```{r, include=F}
# data dimensions
N <- 250
p <- 100
theta <- 0.5
s <- 0.5

generate_data <- function(N, p, theta, s, g, m) {
  # covariance matrix
  i_mat <- matrix(rep(1:p, p), p)
  j_mat <- matrix(rep(1:p, each=p), p)
  Sigma <- s^abs(i_mat - j_mat)
  
  # generate covariates
  X <- mvrnorm(n=N, mu=rep(0, p), Sigma=Sigma)
  
  # generate treatment
  D <- m(X) + rnorm(N)
  
  # generate response
  Y <- D*theta + g(X) + rnorm(N)
  
  data <- as.data.frame(cbind(Y, D, X))
}
```

### Naive ML

A naive approach to estimating $\theta$ would be to estimate $D\theta + g_0(X)$ 
using some machine learning method. In line with the demonstration in 
Chernozhukov et al., we will split the samples into two index sets of equal 
size, $S_1$ (auxiliary) and $S_2$ (primary). We will then use the auxiliary set 
generate the estimate $D\hat{\theta} + \hat{g_0}(X)$, and use the primary set to 
estimate $\theta$ as

$$
\hat{\theta} 
= \left( \frac{1}{n}\sum_{i \in S_2} D_i^2 \right)^{-1} 
\frac{1}{n}\sum_{i \in S_2} D_i(Y_i - \hat{g_0}(X_i)).
$$

```{r, eval=F, echo=F}
n_sim <- 1000
theta.est <- numeric(n_sim)

for (i in 1:n_sim) {
  # obtain new data
  data <- generate_data(N, p, theta, Sigma)
  
  # sample splitting
  idx <- sample.int(N, N/2, replace=F)
  s1 <- data[idx,]
  s2 <- data[-idx,]
  
  # fit random forest
  rf <- randomForest(Y ~ ., ntree=100, data=s1)
  
  # obtain theta estimate
  g.hat <- predict(rf, s2)
  theta.est[i] <- with(s2, mean(D * (Y - g.hat)) / mean(D^2))
}

save(theta.est, file="theta_naive.RData")
```

We do so for the simulated dataset below using random forest regressors. Having 
obtained a vector of 1000 estimates of $\hat{\theta}$, we plot the 
distribution of said estimates to assess whether or not this approach is biased.

```{r, echo=F}
load("theta_naive.RData")

theta.df <- tibble(naive=theta.est) %>%
  pivot_longer(1, names_to="model", values_to="est")

ggplot(aes(x=theta.est), data=theta.df) +
  geom_density(aes(color="grey", fill=model) ,alpha=0.5) +
  geom_vline(aes(xintercept=theta, color="black")) +
  labs(x=TeX("$\\hat{\\theta}$"), y="Density",
       title=TeX("$\\hat{\\theta}$ Distribution (Naive Random Forest)"),
       color="") +
  scale_color_manual(values=c("True Value"="black")) +
  scale_fill_manual(values=c("lightblue"), guide="none") + 
  theme_bw()
```

The estimates of $\theta$ are quite distant from the true value 
of 0.5, indicating this naive ML method for estimating treatment effects is 
biased. Double machine learning was introduced to address this issue.

### Double ML

Instead of estimating $D\theta + g_0(X)$, we may instead first estimate 
$V = D - m_0(X)$ by regressing $D$ on $X$ and obtaining the residuals. 
This will give the estimate $\hat{V} = D - \hat{m_0}(X)$, which we use in 
conjunction with $\hat{g_0}(X)$ to obtain the following estimate of $\theta$:

$$
\hat{\theta} 
= \left( \frac{1}{n}\sum_{i \in S_2} \hat{V}_i D_i \right)^{-1} 
\frac{1}{n}\sum_{i \in S_2} \hat{V}_i(Y_i - \hat{g_0}(X_i)).
$$

The following describes this process in its entirety:

1. Split data into sets $S_1$ and $S_2$
2. Obtain $\hat{g_0}(X)$ via a fit on $S_1$
3. Obtain residuals $\hat{V}=D-\hat{m_0}(X)$ via a fit on $S_1$
4. Estimate $\hat{\theta}$ using $S_2$ by the regression formula above

```{r, eval=F, echo=F}
theta.dml <- numeric(n_sim)

for (i in 1:n_sim) {
  # obtain new data
  data <- generate_data(N, p, theta, Sigma)
  
  # sample splitting
  idx <- sample.int(N, N/2, replace=F)
  s1 <- data[idx,]
  s2 <- data[-idx,]
  
  # fit treatment
  rf.D <- randomForest(D ~ . - Y, ntree=100, data=s1)
  
  # fit response
  rf.Y <- randomForest(Y ~ . - D, ntree=100, data=s1)
  
  # obtain theta estimate
  V.hat <- s2$D - predict(rf.D, s2)
  g.hat <- predict(rf.Y, s2)
  theta.dml[i] <- with(s2, mean(V.hat * (Y - g.hat)) / mean(V.hat * D))
}
```

Having accumulated 1000 samples of $\hat{\theta}$ by the DML method, we may 
again investigate their distribution in comparison to the previous naive method.

```{r, echo=F}
load("theta_dml.RData")

theta.df <- tibble(Naive=theta.est, DML=theta.dml) %>%
  pivot_longer(1:2, names_to="Model", values_to="est")

ggplot(data=theta.df) +
  geom_density(aes(x=est, color="grey", fill=Model), alpha=0.5) +
  geom_vline(aes(xintercept=theta, color="black")) +
  labs(x=TeX("$\\hat{\\theta}$"), y="Density",
       title=TeX("$\\hat{\\theta}$ Distribution (DML + Naive Random Forest)"),
       color="") +
  scale_color_manual(values=c("True Value"="black")) +
  scale_fill_manual(values=c("salmon", "lightblue")) + 
  theme_bw()
```

This time, the distribution appears centered directly around the true value of 
$\theta=0.5$, which indicates that DML was able to avoid bias in treatment 
effect estimation. 

We have therefore been able to replicate the results of Chernozhukov et al., 
demonstrating the utility of double machine learning in producing unbiased 
treatment effect estimates. Notably, the examples above both made use of 
sample splitting in order to produce these estimates. The actual benefit of 
this intermediate step will be the subject of investigation in the following 
section.

# Sample Splitting

Sample splitting entails dividing the data into two groups, $S_1$ and $S_2$,
for function approximation and effect estimation respectively. This was done in 
both examples above in line with the methods introduced in Chernozhukov et al., 
but now we will investigate how important this step is to obtaining unbiased 
treatment effect estimates. 

In this section we will repeat the above DML simulation with and without sample 
splitting. When not using sample splitting, both the function approximation and 
effect estimation will be done using the entire dataset. Additionally, this will
now be done for a range of the number of extraneous covariates and for both 
partial and fully-linear nuisance functions.

### Simple nuisance

For this simulation we will use low-dimensional nuisance functions. The linear 
nuisance functions are defined as:

$$
\begin{aligned}
g_1(X) = \frac{1}{2}X_1 \\
m_1(X) = \frac{1}{8}X_1
\end{aligned}
$$

and the non-linear nuisance functions are defined as:

$$
\begin{aligned}
g_2(X) = \frac{1}{2}\sigma(X_1) \\
m_2(X) = \frac{1}{8}\sigma(X_1).
\end{aligned}
$$

```{r, echo=F}
g_1 <- function(X) {
  1/2 * X[,1]
}

m_1 <- function(X) {
  1/8 * X[,1]
}

g_2 <- function(X) {
  1/2 * sigmoid(X[,1])
}

m_2 <- function(X) {
  1/8 * sigmoid(X[,1])
}
```

```{r, echo=F}
rfDML.split <- function(data) {
  # sample splitting
  idx <- sample.int(N, N/2, replace=F)
  s1 <- data[idx,]
  s2 <- data[-idx,]
  
  # fit treatment
  rf.D.ss <- randomForest(D ~ . - Y, ntree=100, data=s1)
  
  # fit response
  rf.Y.ss <- randomForest(Y ~ . - D, ntree=100, data=s1)
  
  # obtain theta estimate
  V.hat.ss <- s2$D - predict(rf.D.ss, s2)
  g.hat.ss <- predict(rf.Y.ss, s2)
  with(s2, mean(V.hat.ss * (Y - g.hat.ss)) / mean(V.hat.ss * D))
}

rfDML.nosplit <- function(data) {
  # fit treatment
  rf.D.ns <- randomForest(D ~ . - Y, ntree=100, data=data)
  
  # fit response
  rf.Y.ns <- randomForest(Y ~ . - D, ntree=100, data=data)
  
  # obtain theta estimate
  V.hat.ns <- data$D - predict(rf.D.ns, data)
  g.hat.ns <- predict(rf.Y.ns, data)
  with(data, mean(V.hat.ns * (Y - g.hat.ns)) / mean(V.hat.ns * D))
}
```

```{r, eval=F, echo=F}
n_sim <- 300

theta.lin.split <- tibble(
  `p=1`=numeric(n_sim),
  `p=10`=numeric(n_sim),
  `p=100`=numeric(n_sim),
)

theta.lin.nosplit <- tibble(
  `p=1`=numeric(n_sim),
  `p=10`=numeric(n_sim),
  `p=100`=numeric(n_sim),
)

theta.non.split <- tibble(
  `p=1`=numeric(n_sim),
  `p=10`=numeric(n_sim),
  `p=100`=numeric(n_sim),
)

theta.non.nosplit <- tibble(
  `p=1`=numeric(n_sim),
  `p=10`=numeric(n_sim),
  `p=100`=numeric(n_sim),
)

for (i in 1:n_sim) {
  # p=1
  data.lin <- generate_data(N, 1, theta, s, g_1, m_1)
  theta.lin.split[i,1] <- rfDML.split(data.lin)
  theta.lin.nosplit[i,1] <- rfDML.nosplit(data.lin)
  
  data.non <- generate_data(N, 1, theta, s, g_2, m_2)
  theta.non.split[i,1] <- rfDML.split(data.non)
  theta.non.nosplit[i,1] <- rfDML.nosplit(data.non)
  
  # p=10
  data.lin <- generate_data(N, 10, theta, s, g_1, m_1)
  theta.lin.split[i,2] <- rfDML.split(data.lin)
  theta.lin.nosplit[i,2] <- rfDML.nosplit(data.lin)
  
  data.non <- generate_data(N, 10, theta, s, g_2, m_2)
  theta.non.split[i,2] <- rfDML.split(data.non)
  theta.non.nosplit[i,2] <- rfDML.nosplit(data.non)

  # p=100
  data.lin <- generate_data(N, 100, theta, s, g_1, m_1)
  theta.lin.split[i,3] <- rfDML.split(data.lin)
  theta.lin.nosplit[i,3] <- rfDML.nosplit(data.lin)
  
  data.non <- generate_data(N, 100, theta, s, g_2, m_2)
  theta.non.split[i,3] <- rfDML.split(data.non)
  theta.non.nosplit[i,3] <- rfDML.nosplit(data.non)
  
  print(i)
}

save(theta.lin.split, file="theta_lin_split.RData")
save(theta.lin.nosplit, file="theta_lin_nosplit.RData")
save(theta.non.split, file="theta_non_split.RData")
save(theta.non.nosplit, file="theta_non_nosplit.RData")
```

The results for the linear nuisance model are shown below in the form of 
$\hat{\theta}$ distributions for each $p$.

```{r, echo=F, fig.width=12, fig.height=6}
load("theta_lin_split.RData")
load("theta_lin_nosplit.RData")

p1 <- theta.lin.split %>%
  pivot_longer(1:3, names_to="Model", values_to="est") %>%
  ggplot() +
    geom_density(aes(x=est, fill=Model), alpha=0.5) +
    geom_vline(aes(xintercept=theta, color="black")) +
    labs(x=TeX("$\\hat{\\theta}$"), y="Density",
         title=TeX("$\\hat{\\theta}$ Distribution with Sample Splitting"),
         color="") +
    scale_color_manual(values=c("True Value"="black")) +
    scale_fill_manual(values=c("lightgreen", "salmon", "lightblue")) + 
    theme_bw()

p2 <- theta.lin.nosplit %>%
  pivot_longer(1:3, names_to="Model", values_to="est") %>%
  ggplot() +
    geom_density(aes(x=est, fill=Model), alpha=0.5) +
    geom_vline(aes(xintercept=theta, color="black")) +
    labs(x=TeX("$\\hat{\\theta}$"), y="Density",
         title=TeX("$\\hat{\\theta}$ Distribution without Sample Splitting"),
         color="") +
    scale_color_manual(values=c("True Value"="black")) +
    scale_fill_manual(values=c("lightgreen", "salmon", "lightblue")) + 
    theme_bw()

ggarrange(p1, p2, ncol=2, common.legend=T, legend="bottom")
```

It is clear that sample-splitting has a benefit for high-dimensional covariates 
even for an extremely simple relationship between the confounders and 
treatment/response. The benefits of sample splitting are still evident, but much 
less substantial for low-dimensional covariates given the bias observed for 
$p=1$.

We now visualize the results for the simple non-linear nuisance functions.

```{r, echo=F, fig.width=12, fig.height=6}
load("theta_non_split.RData")
load("theta_non_nosplit.RData")

p1 <- theta.non.split %>%
  pivot_longer(1:3, names_to="Model", values_to="est") %>%
  ggplot() +
    geom_density(aes(x=est, fill=Model), alpha=0.5) +
    geom_vline(aes(xintercept=theta, color="black")) +
    labs(x=TeX("$\\hat{\\theta}$"), y="Density",
         title=TeX("$\\hat{\\theta}$ Distribution with Sample Splitting"),
         color="") +
    scale_color_manual(values=c("True Value"="black")) +
    scale_fill_manual(values=c("lightgreen", "salmon", "lightblue")) + 
    theme_bw()

p2 <- theta.non.nosplit %>%
  pivot_longer(1:3, names_to="Model", values_to="est") %>%
  ggplot() +
    geom_density(aes(x=est, fill=Model), alpha=0.5) +
    geom_vline(aes(xintercept=theta, color="black")) +
    labs(x=TeX("$\\hat{\\theta}$"), y="Density",
         title=TeX("$\\hat{\\theta}$ Distribution without Sample Splitting"),
         color="") +
    scale_color_manual(values=c("True Value"="black")) +
    scale_fill_manual(values=c("lightgreen", "salmon", "lightblue")) + 
    theme_bw()

ggarrange(p1, p2, ncol=2, common.legend=T, legend="bottom")
```

These distributions are almost identical to those for the linear nuisance, 
with the benefits of sample-splitting being larger for 
high-dimensional covariates.

### Complex Nuisance

For completeness we will evaluate the benefit of sample splitting for the 
data generating process implemented for the paper replication. This will give an
idea of the performance increase from sample splitting for a more complex 
relationship between the covariates and the treatment/response.

```{r, eval=F, echo=F}
n_sim <- 1000

theta.comp.nosplit <- numeric(n_sim)

for (i in 1:n_sim) {
  # p=100
  data <- generate_data(N, 100, theta, s, g_0, m_0)
  theta.comp.nosplit[i] <-rfDML.nosplit(data)
  print(i)
}

save(theta.comp.nosplit, file="theta_comp_nosplit.RData")
```

```{r, echo=F}
load("theta_comp_nosplit.RData")

theta.df <- tibble(Yes=theta.dml, No=theta.comp.nosplit) %>%
  pivot_longer(1:2, names_to="Splitting?", values_to="est")

ggplot(data=theta.df) +
  geom_density(aes(x=est, color="grey", fill=`Splitting?`), alpha=0.5) +
  geom_vline(aes(xintercept=theta, color="black")) +
  labs(x=TeX("$\\hat{\\theta}$"), y="Density",
       title=TeX("$\\hat{\\theta}$ Distribution (DML + Naive Random Forest)"),
       color="") +
  scale_color_manual(values=c("True Value"="black")) +
  scale_fill_manual(values=c("salmon", "lightblue")) + 
  theme_bw()
```

Again, we observe that bias is reintroduced when we do not use sample splitting 
alongside double machine learning. This is a crucial observation as it could 
mark the difference between a sound treatment effect analysis and a faulty one.
In this particular example we showed that sample splitting is necessary in the 
presence of a higher-dimension non-linear data generating mechanism.

### Cross-Fitting

Cross-fitting is one more technique that we can apply in double ML to achieve 
unbiased treatment effect estimates. Cross-fitting entails taking the weighted 
mean of the estimates generated via sample splitting, with the first estimate 
training on the auxiliary set and estimating on the primary set, and the second 
estimate training on the primary set and estimating on the auxiliary set. Put 
more rigorously, the full process with cross-fitting is the following (where 
superscripts denote the set used to obtain estimates):

1. Split data into sets $S_1$ and $S_2$
2. Obtain $\hat{g_0}^{(1)}(X)$ and $\hat{g_0}^{(2)}(X)$ via $S_1$ and $S_2$ 
respectively
3. Obtain residuals $\hat{V}^{(1)}=D^{(1)}-\hat{m_0}^{(1)}(X^{(1)})$ and 
$\hat{V}^{(2)}=D^{(2)}-\hat{m_0}^{(2)}(X^{(2)})$
4. Estimate $\hat{\theta}^{(1)}$ and $\hat{\theta}^{(2)}$ as shown in the 
previous section
5. Generate final estimate 
$\hat{\theta}=\frac{|S_1|\hat{\theta}^{(1)} + |S_2|\hat{\theta}^{(2)}}{|S_1|+|S_2|}$

```{r, eval=F, echo=F}
theta.cross <- numeric(n_sim)

for (i in 1:n_sim) {
  # obtain new data
  data <- generate_data(N, p, theta, Sigma)
  
  # sample splitting
  idx <- sample.int(N, N/2, replace=F)
  s1 <- data[idx,]
  s2 <- data[-idx,]
  
  # fit treatment
  rf.D.ss.1 <- randomForest(D ~ . - Y, ntree=100, data=s1)
  rf.D.ss.2 <- randomForest(D ~ . - Y, ntree=100, data=s2)
  
  # fit response
  rf.Y.ss.1 <- randomForest(Y ~ . - D, ntree=100, data=s1)
  rf.Y.ss.2 <- randomForest(Y ~ . - D, ntree=100, data=s2)
  
  # obtain theta estimate 1
  V.hat.ss <- s2$D - predict(rf.D.ss.1, s2)
  g.hat.ss <- predict(rf.Y.ss.1, s2)
  theta.1 <- with(s2, mean(V.hat.ss * (Y - g.hat.ss)) / mean(V.hat.ss * D))
  
  # obtain theta estimate 2
  V.hat.ss <- s1$D - predict(rf.D.ss.2, s1)
  g.hat.ss <- predict(rf.Y.ss.2, s1)
  theta.2 <- with(s1, mean(V.hat.ss * (Y - g.hat.ss)) / mean(V.hat.ss * D))
  
  # obtain final estimate
  theta.cross[i] <- mean(c(theta.1, theta.2))
}
```

Finally, we will examine the effect of cross-fitting by evaluating the distributions of the estimates it 
generates.

```{r, echo=F}
load("theta_cross.RData")

theta.df <- tibble(
  Yes=theta.cross, 
  No=theta.dml
) %>%
  pivot_longer(1:2, names_to="Cross-Fitting?", values_to="est")

ggplot(data=theta.df) +
  geom_density(aes(x=est, color="grey", fill=`Cross-Fitting?`), alpha=0.5) +
  geom_vline(aes(xintercept=theta, color="black")) +
  labs(x=TeX("$\\hat{\\theta}$"), y="Density",
       title=TeX("$\\hat{\\theta}$ Distribution - Final Comparison"),
       color="") +
  scale_color_manual(values=c("True Value"="black")) +
  scale_fill_manual(values=c("salmon", "lightblue")) + 
  theme_bw()
```

It is clear that we see marked improvements with the addition of each 
technique. Sample splitting eliminates bias in the treatment effect estimates 
and cross-fitting improves the precision around the true value (as seen in 
the lower dispersion in the distribution of $\hat{\theta}$). As such, we have 
not only demonstrated the necessary value of sample splitting, but the 
additional benefit of cross-fitting in obtaining treatment effect estimates.

# Conclusion

Double machine learning is a powerful technique for treatment effect estimation 
with high-dimensional confounders, such as the case addressed above. While a 
more naive approach would produce biased estimates, DML is able to side-step 
this issue by utilizing a two-pronged machine learning process, in which one 
model is built to predict the response and one is built to predict the treatment 
(both based on the covariates). However, as demonstrated above, the benefits of 
DML will not be realized unless used in conjunction with sample splitting. 
Finally, we showed that estimates with lower uncertainty could be obtained with 
the implementation of cross-fitting.